"""æ•°æ®æŒä¹…åŒ–æ¨¡å— - å°†è®ºæ–‡æ•°æ®å­˜å‚¨ä¸º Parquet æ ¼å¼å¹¶ä¸Šä¼ åˆ° S3"""
import os
import pandas as pd
from pathlib import Path
from datetime import datetime, date
from typing import List
import opendal
import json

from hf import Paper


class PaperStorage:
    """è®ºæ–‡æ•°æ®å­˜å‚¨ç®¡ç†å™¨"""
    
    def __init__(
        self,
        local_data_dir: str = "data",
        s3_bucket: str = None,
        s3_endpoint: str = None,
        s3_region: str = "us-east-1",
        s3_access_key: str = None,
        s3_secret_key: str = None,
    ):
        self.local_data_dir = Path(local_data_dir)
        self.local_data_dir.mkdir(parents=True, exist_ok=True)
        
        # S3 é…ç½®
        self.s3_bucket = s3_bucket
        self.s3_enabled = bool(s3_bucket and s3_access_key and s3_secret_key)
        
        # åˆå§‹åŒ– OpenDAL
        self.operator = None
        if self.s3_enabled:
            try:
                self.operator = opendal.Operator(
                    "s3",
                    bucket=s3_bucket,
                    endpoint=s3_endpoint or "",
                    region=s3_region,
                    access_key_id=s3_access_key,
                    secret_access_key=s3_secret_key,
                )
                print(f"âœ“ S3 å­˜å‚¨å·²é…ç½®: {s3_bucket}")
            except Exception as e:
                print(f"âœ— S3 é…ç½®å¤±è´¥: {e}")
                self.s3_enabled = False
    
    def _paper_to_dict(self, paper: Paper) -> dict:
        """å°† Paper å¯¹è±¡è½¬æ¢ä¸ºå­—å…¸"""
        return {
            'paper_id': paper.get_paper_id(),
            'title': paper.title,
            'authors': json.dumps(paper.authors, ensure_ascii=False),  # è½¬ä¸º JSON å­—ç¬¦ä¸²
            'abstract': paper.abstract,
            'url': str(paper.url),
            'hero_image': str(paper.hero_image) if paper.hero_image else None,
            'arxiv_url': str(paper.arxiv_url) if paper.arxiv_url else None,
            'collected_at': datetime.now().isoformat(),
        }
    
    def save_daily_papers(self, papers: List[Paper], target_date: date):
        """ä¿å­˜æ¯æ—¥è®ºæ–‡æ•°æ®åˆ° Parquet æ–‡ä»¶ï¼ˆå¢é‡æ›´æ–°ï¼‰"""
        if not papers:
            print(f"æ²¡æœ‰è®ºæ–‡æ•°æ®éœ€è¦ä¿å­˜ ({target_date})")
            return None
        
        # ç”Ÿæˆæ–‡ä»¶è·¯å¾„ï¼šdata/YYYY/MM/papers_YYYY-MM-DD.parquet
        year_dir = self.local_data_dir / str(target_date.year)
        month_dir = year_dir / f"{target_date.month:02d}"
        month_dir.mkdir(parents=True, exist_ok=True)
        
        filename = f"papers_{target_date.strftime('%Y-%m-%d')}.parquet"
        filepath = month_dir / filename
        
        # è½¬æ¢æ–°è®ºæ–‡ä¸º DataFrame
        new_data = [self._paper_to_dict(paper) for paper in papers]
        new_df = pd.DataFrame(new_data)
        
        # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼ŒåŠ è½½å¹¶åˆå¹¶å»é‡
        if filepath.exists():
            try:
                existing_df = pd.read_parquet(filepath)
                # åˆå¹¶æ–°æ—§æ•°æ®
                combined_df = pd.concat([existing_df, new_df], ignore_index=True)
                # å»é‡ï¼ˆåŸºäº paper_idï¼Œä¿ç•™ç¬¬ä¸€æ¬¡å‡ºç°çš„ï¼‰
                combined_df = combined_df.drop_duplicates(subset=['paper_id'], keep='first')
                df = combined_df
                print(f"âœ“ åˆå¹¶æ•°æ®: åŸæœ‰ {len(existing_df)} ç¯‡ + æ–°å¢ {len(new_df)} ç¯‡ = å»é‡å {len(df)} ç¯‡")
            except Exception as e:
                print(f"âš ï¸  è¯»å–ç°æœ‰æ–‡ä»¶å¤±è´¥ï¼Œå°†è¦†ç›–: {e}")
                df = new_df
        else:
            df = new_df
            print(f"âœ“ åˆ›å»ºæ–°æ–‡ä»¶: {len(df)} ç¯‡è®ºæ–‡")
        
        # ä¿å­˜ä¸º Parquet æ–‡ä»¶
        df.to_parquet(filepath, engine='pyarrow', compression='snappy', index=False)
        print(f"âœ“ å·²ä¿å­˜åˆ°: {filepath}")
        
        return filepath
    
    def get_monthly_files(self, year: int, month: int) -> List[Path]:
        """è·å–æŒ‡å®šæœˆä»½çš„æ‰€æœ‰ Parquet æ–‡ä»¶"""
        month_dir = self.local_data_dir / str(year) / f"{month:02d}"
        if not month_dir.exists():
            return []
        
        return sorted(month_dir.glob("papers_*.parquet"))
    
    def merge_monthly_data(self, year: int, month: int) -> Path:
        """åˆå¹¶æŒ‡å®šæœˆä»½çš„æ‰€æœ‰æ•°æ®åˆ°ä¸€ä¸ª Parquet æ–‡ä»¶"""
        files = self.get_monthly_files(year, month)
        if not files:
            print(f"æ²¡æœ‰æ‰¾åˆ° {year}-{month:02d} çš„æ•°æ®æ–‡ä»¶")
            return None
        
        # è¯»å–æ‰€æœ‰æ–‡ä»¶å¹¶åˆå¹¶
        dfs = []
        for file in files:
            df = pd.read_parquet(file)
            dfs.append(df)
        
        merged_df = pd.concat(dfs, ignore_index=True)
        
        # å»é‡ï¼ˆåŸºäº paper_idï¼‰
        merged_df = merged_df.drop_duplicates(subset=['paper_id'], keep='first')
        
        # ä¿å­˜åˆå¹¶åçš„æ–‡ä»¶
        merged_filename = f"papers_{year}-{month:02d}_merged.parquet"
        merged_path = self.local_data_dir / str(year) / merged_filename
        merged_df.to_parquet(merged_path, engine='pyarrow', compression='snappy', index=False)
        
        print(f"âœ“ å·²åˆå¹¶ {len(files)} ä¸ªæ–‡ä»¶ï¼Œå…± {len(merged_df)} ç¯‡è®ºæ–‡: {merged_path}")
        return merged_path
    
    def upload_to_s3(self, local_path: Path, s3_key: str = None) -> bool:
        """ä¸Šä¼ æ–‡ä»¶åˆ° S3"""
        if not self.s3_enabled:
            print("S3 æœªé…ç½®ï¼Œè·³è¿‡ä¸Šä¼ ")
            return False
        
        if not local_path.exists():
            print(f"æ–‡ä»¶ä¸å­˜åœ¨: {local_path}")
            return False
        
        # ç”Ÿæˆ S3 key
        if s3_key is None:
            s3_key = str(local_path.relative_to(self.local_data_dir))
        
        try:
            # è¯»å–æ–‡ä»¶å†…å®¹
            with open(local_path, 'rb') as f:
                content = f.read()
            
            # ä¸Šä¼ åˆ° S3
            self.operator.write(s3_key, content)
            print(f"âœ“ å·²ä¸Šä¼ åˆ° S3: s3://{self.s3_bucket}/{s3_key}")
            return True
        
        except Exception as e:
            print(f"âœ— ä¸Šä¼ å¤±è´¥: {e}")
            return False
    
    def archive_month(self, year: int, month: int, delete_daily_files: bool = False) -> bool:
        """å½’æ¡£æœˆåº¦æ•°æ®ï¼šåˆå¹¶å¹¶ä¸Šä¼ åˆ° S3"""
        print(f"\n=== å¼€å§‹å½’æ¡£ {year}-{month:02d} ===")
        
        # 1. åˆå¹¶æœˆåº¦æ•°æ®
        merged_path = self.merge_monthly_data(year, month)
        if not merged_path:
            return False
        
        # 2. ä¸Šä¼ åˆ° S3
        if self.s3_enabled:
            s3_key = f"papers/{year}/{month:02d}/papers_{year}-{month:02d}_merged.parquet"
            success = self.upload_to_s3(merged_path, s3_key)
            
            if success and delete_daily_files:
                # 3. åˆ é™¤æ¯æ—¥æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
                files = self.get_monthly_files(year, month)
                for file in files:
                    try:
                        file.unlink()
                        print(f"âœ“ å·²åˆ é™¤: {file}")
                    except Exception as e:
                        print(f"âœ— åˆ é™¤å¤±è´¥ {file}: {e}")
        
        print(f"=== å½’æ¡£å®Œæˆ {year}-{month:02d} ===\n")
        return True
    
    def load_papers_by_date(self, target_date: date) -> List[dict]:
        """åŠ è½½æŒ‡å®šæ—¥æœŸçš„è®ºæ–‡æ•°æ®"""
        year_dir = self.local_data_dir / str(target_date.year)
        month_dir = year_dir / f"{target_date.month:02d}"
        filename = f"papers_{target_date.strftime('%Y-%m-%d')}.parquet"
        filepath = month_dir / filename
        
        if not filepath.exists():
            return []
        
        df = pd.read_parquet(filepath)
        return df.to_dict('records')
    
    def load_all_paper_ids(self) -> set:
        """ä»æ‰€æœ‰ Parquet æ–‡ä»¶ä¸­åŠ è½½å·²å­˜å‚¨çš„è®ºæ–‡ ID"""
        paper_ids = set()
        
        if not self.local_data_dir.exists():
            return paper_ids
        
        # éå†æ‰€æœ‰å¹´ä»½å’Œæœˆä»½ç›®å½•
        for year_dir in self.local_data_dir.iterdir():
            if not year_dir.is_dir():
                continue
            
            for month_dir in year_dir.iterdir():
                if not month_dir.is_dir():
                    continue
                
                # è¯»å–è¯¥æœˆä»½çš„æ‰€æœ‰ parquet æ–‡ä»¶
                files = list(month_dir.glob("papers_*.parquet"))
                for file in files:
                    try:
                        df = pd.read_parquet(file, columns=['paper_id'])
                        paper_ids.update(df['paper_id'].tolist())
                    except Exception as e:
                        print(f"âš ï¸  è¯»å–æ–‡ä»¶å¤±è´¥ {file}: {e}")
        
        print(f"ğŸ“š ä»å­˜å‚¨ä¸­åŠ è½½äº† {len(paper_ids)} ä¸ªè®ºæ–‡ ID")
        return paper_ids
    
    def get_statistics(self) -> dict:
        """è·å–å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'total_files': 0,
            'total_size_mb': 0,
            'months': {}
        }
        
        for year_dir in self.local_data_dir.iterdir():
            if not year_dir.is_dir():
                continue
            
            for month_dir in year_dir.iterdir():
                if not month_dir.is_dir():
                    continue
                
                files = list(month_dir.glob("papers_*.parquet"))
                total_size = sum(f.stat().st_size for f in files)
                
                month_key = f"{year_dir.name}-{month_dir.name}"
                stats['months'][month_key] = {
                    'files': len(files),
                    'size_mb': round(total_size / 1024 / 1024, 2)
                }
                stats['total_files'] += len(files)
                stats['total_size_mb'] += total_size / 1024 / 1024
        
        stats['total_size_mb'] = round(stats['total_size_mb'], 2)
        return stats


# ç¤ºä¾‹ç”¨æ³•
if __name__ == "__main__":
    from datetime import date
    from hf import fetch_huggingface_papers
    
    # åˆå§‹åŒ–å­˜å‚¨ï¼ˆä¸ä½¿ç”¨ S3ï¼‰
    storage = PaperStorage(
        local_data_dir="data",
        s3_bucket=os.getenv("S3_BUCKET"),
        s3_endpoint=os.getenv("S3_ENDPOINT"),
        s3_access_key=os.getenv("S3_ACCESS_KEY"),
        s3_secret_key=os.getenv("S3_SECRET_KEY"),
    )
    
    # è·å–ä»Šå¤©çš„è®ºæ–‡
    today = date.today()
    print(f"æ­£åœ¨è·å– {today} çš„è®ºæ–‡...")
    papers = fetch_huggingface_papers(today)
    
    # ä¿å­˜åˆ°æœ¬åœ°
    storage.save_daily_papers(papers, today)
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    stats = storage.get_statistics()
    print("\nå­˜å‚¨ç»Ÿè®¡:")
    print(f"  æ€»æ–‡ä»¶æ•°: {stats['total_files']}")
    print(f"  æ€»å¤§å°: {stats['total_size_mb']} MB")
    for month, info in stats['months'].items():
        print(f"  {month}: {info['files']} æ–‡ä»¶, {info['size_mb']} MB")
