"""æ•°æ®æŒä¹…åŒ–æ¨¡å— - å°†è®ºæ–‡æ•°æ®å­˜å‚¨ä¸º Parquet æ ¼å¼"""
import os
import pandas as pd
from pathlib import Path
from datetime import datetime, date
from typing import List, Optional
import opendal
import json

from hf import Paper


class PaperStorage:
    """è®ºæ–‡æ•°æ®å­˜å‚¨ç®¡ç†å™¨
    
    ä½¿ç”¨ OpenDAL æ–‡ä»¶ç³»ç»Ÿå­˜å‚¨æ•°æ®ï¼Œä¾¿äºåç»­æ‰©å±•åˆ°äº‘å­˜å‚¨ã€‚
    å½“å‰ç‰ˆæœ¬ä»…æ”¯æŒæœ¬åœ°æ–‡ä»¶ç³»ç»Ÿã€‚
    """
    
    def __init__(
        self,
        local_data_dir: Optional[str] = None,
        archive_dir: Optional[str] = None,
    ):
        """åˆå§‹åŒ–å­˜å‚¨ç®¡ç†å™¨
        
        Args:
            local_data_dir: æœ¬åœ°æ•°æ®ç›®å½•ï¼ˆé»˜è®¤ä»ç¯å¢ƒå˜é‡ DATA_DIR è¯»å–ï¼Œå¦åˆ™ä¸º "data"ï¼‰
            archive_dir: å½’æ¡£ç›®å½•ï¼ˆé»˜è®¤ä»ç¯å¢ƒå˜é‡ ARCHIVE_DIR è¯»å–ï¼Œå¦åˆ™ä¸º "data/archive"ï¼‰
        """
        # ä»ç¯å¢ƒå˜é‡æˆ–å‚æ•°è·å–ç›®å½•
        if local_data_dir is None:
            local_data_dir = os.getenv("DATA_DIR", "data")
        if archive_dir is None:
            archive_dir = os.getenv("ARCHIVE_DIR", "data/archive")
        
        self.local_data_dir = Path(local_data_dir)
        self.local_data_dir.mkdir(parents=True, exist_ok=True)
        
        # å½’æ¡£ç›®å½•é…ç½®ï¼ˆå§‹ç»ˆç‹¬ç«‹äºæ•°æ®ç›®å½•ï¼‰
        self.archive_dir = Path(archive_dir)
        self.archive_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ– OpenDAL æ–‡ä»¶ç³»ç»Ÿ Operator
        try:
            self.operator = opendal.Operator("fs", root=str(self.archive_dir))
            print(f"âœ“ æ•°æ®ç›®å½•: {self.local_data_dir}")
            print(f"âœ“ å½’æ¡£ç›®å½•: {self.archive_dir}")
        except Exception as e:
            print(f"âš ï¸  OpenDAL åˆå§‹åŒ–å¤±è´¥: {e}")
            print("   å°†ä½¿ç”¨æ ‡å‡†æ–‡ä»¶æ“ä½œ")
            self.operator = None
    
    @classmethod
    def from_env(cls) -> "PaperStorage":
        """ä»ç¯å¢ƒå˜é‡åˆ›å»ºå­˜å‚¨ç®¡ç†å™¨
        
        ç¯å¢ƒå˜é‡:
            DATA_DIR: æ•°æ®ç›®å½•è·¯å¾„ï¼ˆé»˜è®¤: dataï¼‰
            ARCHIVE_DIR: å½’æ¡£ç›®å½•è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        """
        return cls(
            local_data_dir=os.getenv("DATA_DIR"),
            archive_dir=os.getenv("ARCHIVE_DIR")
        )
    
    def _paper_to_dict(self, paper: Paper) -> dict:
        """å°† Paper å¯¹è±¡è½¬æ¢ä¸ºå­—å…¸"""
        return {
            'paper_id': paper.get_paper_id(),
            'title': paper.title,
            'authors': json.dumps(paper.authors, ensure_ascii=False),  # è½¬ä¸º JSON å­—ç¬¦ä¸²
            'abstract': paper.abstract,
            'url': str(paper.url),
            'hero_image': str(paper.hero_image) if paper.hero_image else None,
            'arxiv_url': str(paper.arxiv_url) if paper.arxiv_url else None,
            'github_url': str(paper.github_url) if paper.github_url else None,
            'github_stars': paper.github_stars,
            'hf_upvotes': paper.hf_upvotes,
            'collected_at': datetime.now().isoformat(),
        }
    
    def save_daily_papers(self, papers: List[Paper], target_date: date):
        """ä¿å­˜æ¯æ—¥è®ºæ–‡æ•°æ®åˆ° Parquet æ–‡ä»¶ï¼ˆå¢é‡æ›´æ–°ï¼‰"""
        if not papers:
            print(f"æ²¡æœ‰è®ºæ–‡æ•°æ®éœ€è¦ä¿å­˜ ({target_date})")
            return None
        
        # ç”Ÿæˆæ–‡ä»¶è·¯å¾„ï¼šdata/YYYY/MM/YYYYMMDD.parquet
        year_dir = self.local_data_dir / str(target_date.year)
        month_dir = year_dir / f"{target_date.month:02d}"
        month_dir.mkdir(parents=True, exist_ok=True)
        
        filename = f"{target_date.strftime('%Y%m%d')}.parquet"
        filepath = month_dir / filename
        
        # è½¬æ¢æ–°è®ºæ–‡ä¸º DataFrame
        new_data = [self._paper_to_dict(paper) for paper in papers]
        new_df = pd.DataFrame(new_data)
        
        # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼ŒåŠ è½½å¹¶åˆå¹¶å»é‡
        if filepath.exists():
            try:
                existing_df = pd.read_parquet(filepath)
                # åˆå¹¶æ–°æ—§æ•°æ®
                combined_df = pd.concat([existing_df, new_df], ignore_index=True)
                # å»é‡ï¼ˆåŸºäº paper_idï¼Œä¿ç•™ç¬¬ä¸€æ¬¡å‡ºç°çš„ï¼‰
                combined_df = combined_df.drop_duplicates(subset=['paper_id'], keep='first')
                df = combined_df
                print(f"âœ“ åˆå¹¶æ•°æ®: åŸæœ‰ {len(existing_df)} ç¯‡ + æ–°å¢ {len(new_df)} ç¯‡ = å»é‡å {len(df)} ç¯‡")
            except Exception as e:
                print(f"âš ï¸  è¯»å–ç°æœ‰æ–‡ä»¶å¤±è´¥ï¼Œå°†è¦†ç›–: {e}")
                df = new_df
        else:
            df = new_df
            print(f"âœ“ åˆ›å»ºæ–°æ–‡ä»¶: {len(df)} ç¯‡è®ºæ–‡")
        
        # ä¿å­˜ä¸º Parquet æ–‡ä»¶
        df.to_parquet(filepath, engine='pyarrow', compression='snappy', index=False)
        print(f"âœ“ å·²ä¿å­˜åˆ°: {filepath}")
        
        return filepath
    
    def get_monthly_files(self, year: int, month: int) -> List[Path]:
        """è·å–æŒ‡å®šæœˆä»½çš„æ‰€æœ‰ Parquet æ–‡ä»¶"""
        month_dir = self.local_data_dir / str(year) / f"{month:02d}"
        if not month_dir.exists():
            return []
        
        return sorted(month_dir.glob("*.parquet"))
    
    def merge_monthly_data(self, year: int, month: int) -> Path:
        """åˆå¹¶æŒ‡å®šæœˆä»½çš„æ‰€æœ‰æ•°æ®åˆ°ä¸€ä¸ª Parquet æ–‡ä»¶"""
        files = self.get_monthly_files(year, month)
        if not files:
            print(f"æ²¡æœ‰æ‰¾åˆ° {year}-{month:02d} çš„æ•°æ®æ–‡ä»¶")
            return None
        
        # è¯»å–æ‰€æœ‰æ–‡ä»¶å¹¶åˆå¹¶
        dfs = []
        for file in files:
            df = pd.read_parquet(file)
            dfs.append(df)
        
        merged_df = pd.concat(dfs, ignore_index=True)
        
        # å»é‡ï¼ˆåŸºäº paper_idï¼‰
        merged_df = merged_df.drop_duplicates(subset=['paper_id'], keep='first')
        
        # ä¿å­˜åˆå¹¶åçš„æ–‡ä»¶åˆ°å½’æ¡£ç›®å½•ï¼šYYYYMM.parquet
        merged_filename = f"{year}{month:02d}.parquet"
        
        # ä¿å­˜åˆ°å½’æ¡£ç›®å½•
        archive_year_dir = self.archive_dir / str(year)
        archive_year_dir.mkdir(parents=True, exist_ok=True)
        merged_path = archive_year_dir / merged_filename
        
        merged_df.to_parquet(merged_path, engine='pyarrow', compression='snappy', index=False)
        
        print(f"âœ“ å·²åˆå¹¶ {len(files)} ä¸ªæ–‡ä»¶ï¼Œå…± {len(merged_df)} ç¯‡è®ºæ–‡: {merged_path}")
        return merged_path
    
    def archive_month(self, year: int, month: int, delete_daily_files: bool = False) -> bool:
        """å½’æ¡£æœˆåº¦æ•°æ®ï¼šåˆå¹¶åˆ°å½’æ¡£ç›®å½•
        
        Args:
            year: å¹´ä»½
            month: æœˆä»½
            delete_daily_files: æ˜¯å¦åˆ é™¤æ¯æ—¥æ–‡ä»¶
            
        Returns:
            bool: æ˜¯å¦å½’æ¡£æˆåŠŸ
        """
        print(f"\n=== å¼€å§‹å½’æ¡£ {year}-{month:02d} ===")
        
        # 1. åˆå¹¶æœˆåº¦æ•°æ®
        merged_path = self.merge_monthly_data(year, month)
        if not merged_path:
            return False
        
        # 2. ä½¿ç”¨ OpenDAL å†™å…¥ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.operator:
            try:
                # è¯»å–åˆå¹¶åçš„æ–‡ä»¶
                with open(merged_path, 'rb') as f:
                    content = f.read()
                
                # ä½¿ç”¨ OpenDAL å†™å…¥
                archive_key = f"{year}/{year}{month:02d}.parquet"
                self.operator.write(archive_key, content)
                print(f"âœ“ å·²é€šè¿‡ OpenDAL å†™å…¥å½’æ¡£: {archive_key}")
            except Exception as e:
                print(f"âš ï¸  OpenDAL å†™å…¥å¤±è´¥: {e}")
                print("   æ–‡ä»¶å·²ä¿å­˜åˆ°æœ¬åœ°å½’æ¡£ç›®å½•")
        
        # 3. åˆ é™¤æ¯æ—¥æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
        if delete_daily_files:
            files = self.get_monthly_files(year, month)
            for file in files:
                try:
                    file.unlink()
                    print(f"âœ“ å·²åˆ é™¤æ¯æ—¥æ–‡ä»¶: {file.name}")
                except Exception as e:
                    print(f"âœ— åˆ é™¤å¤±è´¥ {file}: {e}")
        
        print(f"=== å½’æ¡£å®Œæˆ {year}-{month:02d} ===\n")
        return True
    
    def load_papers_by_date(self, target_date: date) -> List[dict]:
        """åŠ è½½æŒ‡å®šæ—¥æœŸçš„è®ºæ–‡æ•°æ®"""
        year_dir = self.local_data_dir / str(target_date.year)
        month_dir = year_dir / f"{target_date.month:02d}"
        filename = f"{target_date.strftime('%Y%m%d')}.parquet"
        filepath = month_dir / filename
        
        if not filepath.exists():
            return []
        
        df = pd.read_parquet(filepath)
        return df.to_dict('records')
    
    def load_all_paper_ids(self) -> set:
        """ä»æ‰€æœ‰ Parquet æ–‡ä»¶ä¸­åŠ è½½å·²å­˜å‚¨çš„è®ºæ–‡ ID"""
        paper_ids = set()
        
        if not self.local_data_dir.exists():
            return paper_ids
        
        # éå†æ‰€æœ‰å¹´ä»½å’Œæœˆä»½ç›®å½•
        for year_dir in self.local_data_dir.iterdir():
            if not year_dir.is_dir():
                continue
            
            for month_dir in year_dir.iterdir():
                if not month_dir.is_dir():
                    continue
                
                # è¯»å–è¯¥æœˆä»½çš„æ‰€æœ‰ parquet æ–‡ä»¶ï¼ˆæ’é™¤æœˆåº¦å½’æ¡£æ–‡ä»¶ï¼‰
                files = [f for f in month_dir.glob("*.parquet") if len(f.stem) == 8]  # YYYYMMDD æ ¼å¼
                for file in files:
                    try:
                        df = pd.read_parquet(file, columns=['paper_id'])
                        paper_ids.update(df['paper_id'].tolist())
                    except Exception as e:
                        print(f"âš ï¸  è¯»å–æ–‡ä»¶å¤±è´¥ {file}: {e}")
        
        print(f"ğŸ“š ä»å­˜å‚¨ä¸­åŠ è½½äº† {len(paper_ids)} ä¸ªè®ºæ–‡ ID")
        return paper_ids
    
    def get_statistics(self) -> dict:
        """è·å–å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'total_files': 0,
            'total_size_mb': 0,
            'months': {}
        }
        
        for year_dir in self.local_data_dir.iterdir():
            if not year_dir.is_dir():
                continue
            
            for month_dir in year_dir.iterdir():
                if not month_dir.is_dir():
                    continue
                
                files = list(month_dir.glob("papers_*.parquet"))
                total_size = sum(f.stat().st_size for f in files)
                
                month_key = f"{year_dir.name}-{month_dir.name}"
                stats['months'][month_key] = {
                    'files': len(files),
                    'size_mb': round(total_size / 1024 / 1024, 2)
                }
                stats['total_files'] += len(files)
                stats['total_size_mb'] += total_size / 1024 / 1024
        
        stats['total_size_mb'] = round(stats['total_size_mb'], 2)
        return stats


# ç¤ºä¾‹ç”¨æ³•
if __name__ == "__main__":
    from datetime import date
    from hf import fetch_huggingface_papers
    
    # åˆå§‹åŒ–å­˜å‚¨ï¼ˆä»ç¯å¢ƒå˜é‡åŠ è½½é…ç½®ï¼‰
    storage = PaperStorage.from_env()
    
    # è·å–ä»Šå¤©çš„è®ºæ–‡
    today = date.today()
    print(f"æ­£åœ¨è·å– {today} çš„è®ºæ–‡...")
    papers = fetch_huggingface_papers(today)
    
    # ä¿å­˜åˆ°æœ¬åœ°
    storage.save_daily_papers(papers, today)
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    stats = storage.get_statistics()
    print("\nå­˜å‚¨ç»Ÿè®¡:")
    print(f"  æ€»æ–‡ä»¶æ•°: {stats['total_files']}")
    print(f"  æ€»å¤§å°: {stats['total_size_mb']} MB")
    for month, info in stats['months'].items():
        print(f"  {month}: {info['files']} æ–‡ä»¶, {info['size_mb']} MB")
